
import pandas as pd
import matplotlib.pyplot as plt
import numpy as np
import yaml
import subprocess
import os
import shutil
import glob
from mpl_toolkits.mplot3d import Axes3D  # noqa: F401


SPIKE_WEIGHTS = [0, 0, 0, 0, 1]
EPSILON_DEFAULT = 1.5
G_MAX = 2_000_000
TARGET_TOTAL_DEMAND = G_MAX / 2

EPSILON_MIN = 0.0
EPSILON_MAX = 16.0
EPSILON_STEP = 0.1

ALPHA_MIN = 0.01
ALPHA_MAX = 0.99
ALPHA_STEP = 0.01

LAMBDA_DEFAULT = 1.5  

EPSILON_POINTS = int(round((EPSILON_MAX - EPSILON_MIN) / EPSILON_STEP)) + 1
ALPHA_POINTS  = int(round((ALPHA_MAX - ALPHA_MIN) / ALPHA_STEP)) + 1



def load_config():
    try:
        with open('config.yml', 'r') as f:
            return yaml.safe_load(f)
    except FileNotFoundError:
        print("‚ùå config.yml not found")
        return None

def backup_config():
    try:
        shutil.copy('config.yml', 'config_backup.yml')
        print("‚úÖ Created config backup")
        return True
    except Exception as e:
        print(f"‚ùå Failed to backup config: {e}")
        return False

def restore_config():
    try:
        shutil.copy('config_backup.yml', 'config.yml')
        print("‚úÖ Restored original config")
        return True
    except Exception as e:
        print(f"‚ùå Failed to restore config: {e}")
        return False



def _detect_num_shards_from_config(cfg):
    try:
        # ÂÖàÁúã demand.base_demand_matrix
        m = cfg.get('demand', {}).get('base_demand_matrix')
        if isinstance(m, list) and m and all(isinstance(r, list) and len(r) == len(m) for r in m):
            return len(m)
    except Exception:
        pass

    for path in [('network','num_shards'), ('demand','num_shards'), ('system','num_shards'), ('num_shards',)]:
        try:
            cur = cfg
            for k in path:
                cur = cur[k]
            if isinstance(cur, int) and cur >= 3:
                return cur
        except Exception:
            continue
    return 3


def _ensure_network_and_shards(cfg, N):
    # network.num_shards
    cfg.setdefault('network', {})['num_shards'] = N

    shards = [{'id': i, 'g_max': G_MAX} for i in range(N)]
    cfg['shards'] = shards



def calculate_demand_matrix(alpha_inflow_to_0, num_shards):

    alpha = float(alpha_inflow_to_0)
    if not (0.0 <= alpha <= 1.0):
        raise ValueError(f"alpha_inflow_to_0 must be in [0,1]Ôºånow={alpha}")
    N = int(num_shards)
    if N < 3:
        raise ValueError("num_shards must ‚â• 3")

    T = TARGET_TOTAL_DEMAND
    d = (1.0 - alpha) * T                
    a0 = (alpha / (N - 1)) * T            
    x  = (alpha / (2 * (N - 1))) * T      

    M = [[0.0 for _ in range(N)] for __ in range(N)]


    for i in range(N):
        M[i][i] = d

    for k in range(1, N):
        M[0][k] = 0.0

    for i in range(1, N):
        M[i][0] = a0

    for i in range(1, N):
        for j in range(1, N):
            if i != j:
                M[i][j] = x

    return M


def verify_demand_matrix(matrix, alpha_value):
    N = len(matrix)
    T = TARGET_TOTAL_DEMAND
    tol = 1e-6

    inbound_to_0 = sum(matrix[i][0] for i in range(1, N))
    cond1 = abs(inbound_to_0 - alpha_value * T) < tol
    cond2 = abs(matrix[0][0] - (1 - alpha_value) * T) < tol
    cond3 = all(abs(matrix[0][k]) < 1e-12 for k in range(1, N))
    diag = [matrix[i][i] for i in range(N)]
    cond4 = all(abs(di - diag[0]) < tol for di in diag)

    def inbound(i):  return sum(matrix[j][i] for j in range(N) if j != i)
    def outbound(i): return sum(matrix[i][j] for j in range(N) if j != i)

    per_shard_ok = []
    for i in range(N):
        s = inbound(i) + outbound(i) + matrix[i][i]
        per_shard_ok.append(abs(s - T) < tol)

    nonneg = all(matrix[i][j] >= -1e-12 for i in range(N) for j in range(N))

    is_valid = cond1 and cond2 and cond3 and cond4 and all(per_shard_ok) and nonneg

    print("Demand matrix verification:")
    print(f"  N = {N}, alpha = {alpha_value:.4f}, TARGET_TOTAL_DEMAND = {T:.0f}")
    print(f"  inbound‚Üí0 = {inbound_to_0:.6f} (target {alpha_value*T:.6f})  -> {cond1}")
    print(f"  A00 = {matrix[0][0]:.6f} (target {(1-alpha_value)*T:.6f})    -> {cond2}")
    print(f"  A0k(k>0)=0                                                  -> {cond3}")
    print(f"  diagonals equal                                             -> {cond4}")
    for i in range(N):
        s = inbound(i) + outbound(i) + matrix[i][i]
        print(f"  shard {i}: inbound+outbound+local = {s:.6f} (target {T:.6f}) -> {per_shard_ok[i]}")
    print(f"  non-negative                                                -> {nonneg}")
    print(f"  Constraints satisfied: {is_valid}")

    return is_valid



def update_config_for_experiment(epsilon_j0, alpha_inflow, delay_weights, num_shards):

    try:
        cfg = load_config()
        if cfg is None:
            return False

        N = int(num_shards)

        # network / shards
        _ensure_network_and_shards(cfg, N)


        cfg.setdefault('demand', {})


        em = [[(0.0 if i == j else EPSILON_DEFAULT) for j in range(N)] for i in range(N)]
        for i in range(1, N):
            em[i][0] = float(epsilon_j0)  
        cfg['demand']['epsilon_matrix'] = em

        # base_demand_matrix
        bdm = calculate_demand_matrix(alpha_inflow, num_shards=N)
        cfg['demand']['base_demand_matrix'] = bdm

        lam_val = LAMBDA_DEFAULT
        try:
            old_lm = cfg['demand'].get('lambda_matrix')
            if isinstance(old_lm, list) and old_lm and isinstance(old_lm[0], list) and old_lm[0]:
                lam_val = float(old_lm[0][0])
        except Exception:
            pass
        lm = [[lam_val for _ in range(N)] for __ in range(N)]
        cfg['demand']['lambda_matrix'] = lm


        cfg.setdefault('delay', {})['weights'] = delay_weights

  
        cfg.setdefault('system', {})['num_shards'] = N
        cfg.setdefault('demand', {})['num_shards'] = N

        with open('config.yml', 'w') as f:
            yaml.dump(cfg, f, default_flow_style=False)

        print(f"‚úÖ Config updated (N={N}): epsilon_j0={epsilon_j0:.2f}, alpha={alpha_inflow:.2f}")
        return True

    except Exception as e:
        print(f"‚ùå Failed to update config: {e}")
        return False

def run_simulation():
    try:
        result = subprocess.run(['go', 'run', '../../../main.go'],
                                capture_output=True, text=True, timeout=120)
        if result.returncode == 0:
            return True
        else:
            print(f"‚ùå Simulation failed: {result.stderr}")
            return False
    except subprocess.TimeoutExpired:
        print("‚ùå Simulation timed out")
        return False
    except Exception as e:
        print(f"‚ùå Error running simulation: {e}")
        return False

def find_latest_log():
    logs = glob.glob("enhanced_simulation_analysis_*.log")
    return max(logs, key=os.path.getctime) if logs else None

def parse_final_load(log_file):
    try:
        with open(log_file, 'r') as f:
            content = f.read()

        print(f"  üìÑ Parsing log file: {log_file}")

        data_start_idx = content.find('DATA_START')
        if data_start_idx == -1:
            print(f"  ‚ùå No DATA_START found in {log_file}")
            return None

        lines = content[data_start_idx:].split('\n')
        csv_data, header_found = [], False
        for line in lines:
            s = line.strip()
            if s.startswith('Step,Shard0_Fee'):
                header_found = True
                csv_data.append(s)
            elif header_found and ',' in s and not s.startswith('#'):
                parts = s.split(',')
                if len(parts) >= 7:
                    try:
                        int(parts[0])
                        csv_data.append(s)
                    except ValueError:
                        break

        if len(csv_data) <= 1:
            print("  ‚ùå No valid CSV data found")
            return None

        from io import StringIO
        df = pd.read_csv(StringIO('\n'.join(csv_data)))
        shard0_loads = df['Shard0_Load']
        final_load = shard0_loads.iloc[-1]
        avg_load   = shard0_loads.mean()
        load_std   = shard0_loads.std()
        target     = 0.5
        load_infl  = (avg_load - target) / target

        print(f"  ‚úÖ Parsed {len(shard0_loads)} steps of shard0 data")
        print(f"  üìä Shard0 stats: final={final_load:.6f}, avg={avg_load:.6f}, std={load_std:.6f}")

        return {
            'load0': final_load,
            'avg_load': avg_load,
            'load_std': load_std,
            'load_inflation': load_infl
        }

    except Exception as e:
        print(f"‚ùå Error parsing log: {e}")
        return {
            'load0': 0.5,
            'avg_load': 0.5,
            'load_std': 0.0,
            'load_inflation': 0.0
        }

def clean_log_files():
    try:
        logs = glob.glob("enhanced_simulation_analysis_*.log") + glob.glob("experiment_*.log")
        for p in logs:
            os.remove(p)
        if logs:
            print(f"  üßπ Cleaned {len(logs)} remaining log files")
        return True
    except Exception as e:
        print(f"‚ö†Ô∏è  Warning: Could not clean log files: {e}")
        return False

def check_convergence(load_stats, tolerance=1e-5):
    if load_stats is None or not isinstance(load_stats, dict):
        return False
    return abs(load_stats['load0'] - 0.5) < tolerance


def run_single_experiment(epsilon_j0, alpha_inflow, delay_weights, exp_id, num_shards):
    if not update_config_for_experiment(epsilon_j0, alpha_inflow, delay_weights, num_shards=num_shards):
        return None

    unique_log_name = f"experiment_Spike_{exp_id:06d}_N{num_shards}_eps{epsilon_j0:.2f}_alpha{alpha_inflow:.2f}.log"

    if not run_simulation():
        return None

    latest_log = find_latest_log()
    if latest_log is None:
        print(f"  ‚ùå No log file found for experiment {exp_id}")
        return None

    try:
        os.rename(latest_log, unique_log_name)
        print(f"  üìù Renamed log to: {unique_log_name}")
    except Exception:
        unique_log_name = latest_log

    load_stats = parse_final_load(unique_log_name)
    converged = check_convergence(load_stats)

    try:
        os.remove(unique_log_name)
        print(f"  üóëÔ∏è  Cleaned up: {unique_log_name}")
    except Exception:
        pass

    return {
        'num_shards': num_shards,
        'epsilon_j0': epsilon_j0,
        'alpha_inflow': alpha_inflow,
        'distribution': 'Spike',
        'final_load': load_stats['load0'] if load_stats else None,
        'avg_load': load_stats['avg_load'] if load_stats else None,
        'load_std': load_stats['load_std'] if load_stats else None,
        'load_inflation': load_stats['load_inflation'] if load_stats else None,
        'converged': converged
    }


def save_distribution_results(results, csv_filename):
    df = pd.DataFrame(results)
    df.to_csv(csv_filename, index=False)
    print(f"üíæ Saved {len(results)} results to {csv_filename}")



def run_convergence_experiment():
    print("üî¨ Starting Convergence Boundary Experiment (Spike only, multi-N)")
    print("=" * 80)
    shard_list = [10, 7, 5]
    delay_weights = SPIKE_WEIGHTS

    total_points = EPSILON_POINTS * ALPHA_POINTS * len(shard_list)
    print(f"Total experiments to run: {total_points}")
    print(f"Epsilon range: {EPSILON_MIN}~{EPSILON_MAX} (step {EPSILON_STEP}, {EPSILON_POINTS} pts)")
    print(f"Alpha   range: {ALPHA_MIN}~{ALPHA_MAX} (step {ALPHA_STEP}, {ALPHA_POINTS} pts)")
    print(f"Shards to run: {shard_list} (order)")
    print(f"Latency dist.: Spike {delay_weights}")

    if not backup_config():
        print("‚ùå Failed to backup config, aborting")
        return

    experiment_count = 0

    try:
        for N in shard_list:
            print(f"\nüß™ Running Spike with N={N}")
            print("-" * 60)
            results_N = []

            for i in range(EPSILON_POINTS - 1, -1, -1):
                epsilon_j0 = EPSILON_MIN + i * EPSILON_STEP
                for j in range(ALPHA_POINTS - 1, -1, -1):
                    alpha_inflow = ALPHA_MIN + j * ALPHA_STEP
                    experiment_count += 1

                    print(f"Experiment {experiment_count}/{total_points}: N={N}, Œµ={epsilon_j0:.2f}, Œ±={alpha_inflow:.2f}")

                    r = run_single_experiment(
                        epsilon_j0, alpha_inflow,
                        delay_weights=delay_weights,
                        exp_id=experiment_count,
                        num_shards=N
                    )
                    if r is not None:
                        results_N.append(r)
                        status = "‚úÖ Converged" if r['converged'] else "‚ùå Diverged"
                        print(f"  ‚Üí Load0: {r['final_load']:.6f}, Avg: {r['avg_load']:.6f}, Std: {r['load_std']:.6f}, Infl: {r['load_inflation']:.6f}, {status}")
                    else:
                        print("  ‚Üí ‚ùå Experiment failed")
                        results_N.append({
                            'num_shards': N,
                            'epsilon_j0': epsilon_j0,
                            'alpha_inflow': alpha_inflow,
                            'distribution': 'Spike',
                            'final_load': None,
                            'avg_load': None,
                            'load_std': None,
                            'load_inflation': None,
                            'converged': False
                        })

                    if len(results_N) % 10 == 0:
                        save_distribution_results(results_N, f"convergence_results_spike_shard{N}.csv")

                    if experiment_count % 50 == 0:
                        clean_log_files()

            save_distribution_results(results_N, f"convergence_results_spike_shard{N}.csv")
            print(f"‚úÖ Completed N={N}: {len(results_N)} experiments")

        print("\nüéâ All Spike experiments for N‚àà{10,7,5} completed!")

    finally:
        restore_config()
        print("üîÑ Original config.yml restored")



def load_and_combine_results():
    all_results, files = [], []
    for N in [10, 7, 5]:
        fn = f"convergence_results_spike_shard{N}.csv"
        if os.path.exists(fn):
            files.append(fn)
    print(f"Found {len(files)} spike result files")
    for fp in files:
        try:
            df = pd.read_csv(fp)
            all_results.append(df)
            print(f"Loaded {len(df)} results from {fp}")
        except Exception as e:
            print(f"‚ùå Error loading {fp}: {e}")
    if all_results:
        combined = pd.concat(all_results, ignore_index=True)
        print(f"‚úÖ Combined {len(combined)} total results")
        return combined
    return None

def create_3d_convergence_plots(df):
    if df is None or len(df) == 0:
        print("‚ùå No data available for plotting")
        return
    shard_values = sorted(df['num_shards'].dropna().unique())[::-1]
    fig = plt.figure(figsize=(6 * len(shard_values), 6))
    for i, N in enumerate(shard_values):
        dist_data = df[df['num_shards'] == N]
        ax = fig.add_subplot(1, len(shard_values), i + 1, projection='3d')
        converged = dist_data[dist_data['converged'] == True]
        diverged  = dist_data[dist_data['converged'] == False]
        if len(converged) > 0:
            ax.scatter(converged['epsilon_j0'], converged['alpha_inflow'], [1]*len(converged),
                       c='green', marker='o', s=20, alpha=0.6, label='Converged')
        if len(diverged) > 0:
            ax.scatter(diverged['epsilon_j0'], diverged['alpha_inflow'], [0]*len(diverged),
                       c='red', marker='x', s=20, alpha=0.6, label='Diverged')
        ax.set_xlabel('Epsilon (Œµ_j0)')
        ax.set_ylabel('Alpha (Œ±_j‚Üí0,in)')
        ax.set_zlabel('Convergence')
        ax.set_title(f'N={N} (Spike)')
        ax.set_zlim(-0.1, 1.1)
        ax.set_zticks([0, 1])
        ax.set_zticklabels(['Diverged', 'Converged'])
        ax.legend()
        ax.view_init(elev=20, azim=45)
    plt.tight_layout()
    plt.savefig('convergence_boundary_spike_multiN_3d.png', dpi=300, bbox_inches='tight')
    plt.savefig('convergence_boundary_spike_multiN_3d.pdf', dpi=300, bbox_inches='tight')
    print("üíæ 3D convergence plots saved as convergence_boundary_spike_multiN_3d.png/pdf")
    plt.show()

def analyze_convergence_results(df):
    if df is None or len(df) == 0:
        print("‚ùå No data available for analysis")
        return
    print("\nüìä CONVERGENCE ANALYSIS SUMMARY (Spike)")
    print("=" * 80)
    for N in sorted(df['num_shards'].dropna().unique())[::-1]:
        dist_data = df[df['num_shards'] == N]
        total = len(dist_data)
        conv  = len(dist_data[dist_data['converged'] == True])
        rate  = conv / total * 100
        print(f"\nüî¨ N={N}:")
        print(f"   Total experiments: {total}")
        print(f"   Converged: {conv} ({rate:.1f}%)")
        print(f"   Diverged: {total - conv} ({100 - rate:.1f}%)")
        csub = dist_data[dist_data['converged'] == True]
        if len(csub) > 0:
            print(f"   Max converged Œµ: {csub['epsilon_j0'].max():.2f}")
            print(f"   Max converged Œ±: {csub['alpha_inflow'].max():.2f}")
    out = "convergence_analysis_spike_summary.csv"
    stat = []
    for N in sorted(df['num_shards'].dropna().unique())[::-1]:
        dist_data = df[df['num_shards'] == N]
        total = len(dist_data)
        conv  = len(dist_data[dist_data['converged'] == True])
        stat.append({'num_shards': int(N), 'total_experiments': int(total),
                     'converged_count': int(conv), 'convergence_rate': conv / total * 100})
    pd.DataFrame(stat).to_csv(out, index=False)
    print(f"\nüíæ Summary saved to {out}")

def visualize_results():
    print("üìà Loading and visualizing Spike results...")
    df = load_and_combine_results()
    if df is not None:
        analyze_convergence_results(df)
        create_3d_convergence_plots(df)
        df.to_csv("convergence_results_spike_combined.csv", index=False)
        print("üíæ Combined results saved to convergence_results_spike_combined.csv")
    else:
        print("‚ùå No results to visualize")


if __name__ == "__main__":
    print("üî¨ Convergence Boundary Experiment (Spike-only, multi-shard: N=10,7,5)")
    print("=" * 60)
    print("Choose operation:")
    print("1. Test demand matrix calculation (N=3,5,7,10)")
    print("2. Run Spike experiment for N=10,7,5")
    print("3. Visualize existing Spike results")
    print("4. Exit")
    choice = input("Enter choice (1-4): ").strip()

    if choice == "1":
        okN = True
        for N in [3,5,7,10]:
            for a in [0.10, 0.25, 0.50, 0.75, 0.90]:
                M = calculate_demand_matrix(a, num_shards=N)
                if not verify_demand_matrix(M, a):
                    okN = False
                    break
        print("\n‚úÖ Demand matrix calculation verified!" if okN else "\n‚ùå Demand matrix check failed.")

    elif choice == "2":

        M = calculate_demand_matrix(0.7, num_shards=5)
        if verify_demand_matrix(M, 0.7):
            run_convergence_experiment()
        else:
            print("\n‚ùå Demand matrix calculation has issues, aborting.")

    elif choice == "3":
        visualize_results()

    elif choice == "4":
        print("Goodbye!")
    else:
        print("Invalid choice. Please run again.")
